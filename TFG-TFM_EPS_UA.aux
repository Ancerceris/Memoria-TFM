\relax 
\providecommand\zref@newlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\ProvideDocumentCommand \xpg@aux {mm}{}
\HyPL@Entry{0<</S/r>>}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{TFG-TFM_EPS_UA.ist}
\@glsorder{word}
\@writefile{toc}{\xpg@aux {}{spanish}}
\oddpage@label{1}{1}
\savepicturepage{pgfid1}{1}
\pgfsyspdfmark {pgfid1}{4685945}{10261346}
\@writefile{toc}{\contentsline {section}{\numberline {1}Rationale for an Automated, Deep Learning-Based Framework}{v}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Justification of the Two-Stage Analysis Pipeline}{v}{section.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Stage 1: From Bounding Boxes to Markerless Pose-Estimation}{vi}{subsection.0.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Stage 2: Feature Engineering and Supervised Classification}{vi}{subsection.0.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Justification of the Core Technology Stack}{vii}{section.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Justification for an Interactive Visualization Platform}{vii}{section.0.4}\protected@file@percent }
\HyPL@Entry{20<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Overview}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation}{1}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Proposal and goals}{2}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Timeline}{3}{section.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces \xpg@aux {}{spanish}Gantt chart illustrating the timeline of the thesis project.}}{3}{figure.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gantt-chart}{{1.1}{3}{\xpg@aux {\xpg@current@opts }{\languagename }Gantt chart illustrating the timeline of the thesis project}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Learning stage}{3}{subsection.1.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces \xpg@aux {}{spanish}Learning stage: foundational knowledge acquisition.}}{4}{figure.caption.9}\protected@file@percent }
\newlabel{fig:learning-stage}{{1.2}{4}{\xpg@aux {\xpg@current@opts }{\languagename }Learning stage: foundational knowledge acquisition}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Research stage}{4}{subsection.1.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces \xpg@aux {}{spanish}Research stage: in-depth exploration of models and techniques.}}{4}{figure.caption.10}\protected@file@percent }
\newlabel{fig:research-stage}{{1.3}{4}{\xpg@aux {\xpg@current@opts }{\languagename }Research stage: in-depth exploration of models and techniques}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Implementation stage}{4}{subsection.1.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces \xpg@aux {}{spanish}Implementation stage: practical realization of the system.}}{5}{figure.caption.11}\protected@file@percent }
\newlabel{fig:implementation-stage}{{1.4}{5}{\xpg@aux {\xpg@current@opts }{\languagename }Implementation stage: practical realization of the system}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Experimentation stage}{5}{subsection.1.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces \xpg@aux {}{spanish}Experimentation stage: performance assessment of the system.}}{5}{figure.caption.12}\protected@file@percent }
\newlabel{fig:experimentation-stage}{{1.5}{5}{\xpg@aux {\xpg@current@opts }{\languagename }Experimentation stage: performance assessment of the system}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Outline}{5}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}State of the Art}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\newlabel{marcoteorico}{{2}{7}{State of the Art}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}CNNs and Transformers for Video Analysis}{7}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Inductive Bias of Convolutional Neural Networks (CNNs)}{7}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \xpg@aux {}{spanish}The convolution operation, illustrating the local processing nature of CNNs.}}{8}{figure.caption.13}\protected@file@percent }
\newlabel{fig:convolution}{{2.1}{8}{\xpg@aux {\xpg@current@opts }{\languagename }The convolution operation, illustrating the local processing nature of CNNs}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Vision Transformers (ViTs) and the Global Context}{8}{subsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \xpg@aux {}{spanish}The detailed architecture of a standard Vision Transformer (ViT).}}{9}{figure.caption.14}\protected@file@percent }
\newlabel{fig:vit_arch}{{2.2}{9}{\xpg@aux {\xpg@current@opts }{\languagename }The detailed architecture of a standard Vision Transformer (ViT)}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}The Synthesis: Hybrid Vision Transformers (HVTs)}{9}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \xpg@aux {}{spanish}A taxonomy of Hybrid Vision Transformer architectures, classifying models based on their integration strategy.}}{10}{figure.caption.15}\protected@file@percent }
\newlabel{fig:hvt_taxonomy}{{2.3}{10}{\xpg@aux {\xpg@current@opts }{\languagename }A taxonomy of Hybrid Vision Transformer architectures, classifying models based on their integration strategy}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}CNN-Transformer Integration Architectures}{10}{subsection.2.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \xpg@aux {}{spanish}An example of a sequential integration architecture: CMT.}}{10}{figure.caption.16}\protected@file@percent }
\newlabel{fig:cmt_arch}{{2.4}{10}{\xpg@aux {\xpg@current@opts }{\languagename }An example of a sequential integration architecture: CMT}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Overview of Deep Learning for Video Scene Analysis}{11}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Convolutional Neural Networks (CNNs) for Spatial Understanding}{11}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces \xpg@aux {}{spanish}An illustration of a human object recognition system using a CNN.}}{12}{figure.caption.17}\protected@file@percent }
\newlabel{fig:cnn_recognition}{{2.5}{12}{\xpg@aux {\xpg@current@opts }{\languagename }An illustration of a human object recognition system using a CNN}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Recurrent Neural Networks (RNNs) for Temporal Dynamics}{12}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Hybrid CNN-RNN Models: The Standard for Activity Recognition}{12}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \xpg@aux {}{spanish}The architecture of a hybrid CNN-RNN system for human activity recognition.}}{13}{figure.caption.18}\protected@file@percent }
\newlabel{fig:cnn_rnn_activity}{{2.6}{13}{\xpg@aux {\xpg@current@opts }{\languagename }The architecture of a hybrid CNN-RNN system for human activity recognition}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}A Taxonomy of Video Analysis Applications}{13}{section.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces \xpg@aux {}{spanish}An application-based taxonomy of deep learning methods used in Deep Vision Systems (DVS).}}{14}{figure.caption.19}\protected@file@percent }
\newlabel{fig:taxonomy}{{2.7}{14}{\xpg@aux {\xpg@current@opts }{\languagename }An application-based taxonomy of deep learning methods used in Deep Vision Systems (DVS)}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}3D Convolutional Neural Networks for Human Action Recognition}{15}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Limitations of 2D CNNs for Action Recognition}{15}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}The 3D Convolution Operation}{16}{subsection.2.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces \xpg@aux {}{spanish}A visual comparison of (a) 2D convolution and (b) 3D convolution.}}{17}{figure.caption.20}\protected@file@percent }
\newlabel{fig:2d_vs_3d}{{2.8}{17}{\xpg@aux {\xpg@current@opts }{\languagename }A visual comparison of (a) 2D convolution and (b) 3D convolution}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Learning Multiple Feature Types}{18}{subsection.2.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces \xpg@aux {}{spanish}Extraction of multiple features using multiple distinct 3D convolution kernels.}}{18}{figure.caption.21}\protected@file@percent }
\newlabel{fig:multiple_features}{{2.9}{18}{\xpg@aux {\xpg@current@opts }{\languagename }Extraction of multiple features using multiple distinct 3D convolution kernels}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}A 3D CNN Architecture for Action Recognition}{18}{subsection.2.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \xpg@aux {}{spanish}A complete 3D CNN architecture for human action recognition.}}{19}{figure.caption.22}\protected@file@percent }
\newlabel{fig:3d_arch}{{2.10}{19}{\xpg@aux {\xpg@current@opts }{\languagename }A complete 3D CNN architecture for human action recognition}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Implications for Video Analysis}{19}{subsection.2.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}SimBA: Simple Behavioral Analysis Toolkit}{19}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}The Challenge and Limitations of Manual Behavioral Annotation}{20}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}The Paradigm Shift: From Pixels to Pose}{20}{subsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Pose-Estimation with Deep Learning}{20}{subsection.2.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces \xpg@aux {}{spanish}Animal pose-estimation body-part tracking. (a) A schematic of 16 body parts tracked on two mice. (c) The flexible interface in SimBA for defining custom tracking schemas. (d) Examples of different tracking schemas. This figure is fundamental to understanding that the initial data for behavior classification is the animal's skeleton, not the raw image.}}{21}{figure.caption.23}\protected@file@percent }
\newlabel{fig:pose_estimation}{{2.11}{21}{\xpg@aux {\xpg@current@opts }{\languagename }Animal pose-estimation body-part tracking. (a) A schematic of 16 body parts tracked on two mice. (c) The flexible interface in SimBA for defining custom tracking schemas. (d) Examples of different tracking schemas. This figure is fundamental to understanding that the initial data for behavior classification is the animal's skeleton, not the raw image}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}From Pose Data to Behavioral Features}{21}{subsection.2.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Supervised Classification of Behaviors}{21}{subsection.2.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.6}The SimBA Workflow: Ensuring Accessibility and Transparency}{22}{subsection.2.5.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces \xpg@aux {}{spanish}The complete SimBA workflow for creating supervised machine learning classifiers.}}{22}{figure.caption.24}\protected@file@percent }
\newlabel{fig:simba_workflow}{{2.12}{22}{\xpg@aux {\xpg@current@opts }{\languagename }The complete SimBA workflow for creating supervised machine learning classifiers}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces \xpg@aux {}{spanish}Feature permutation importances for classifiers.}}{23}{figure.caption.25}\protected@file@percent }
\newlabel{fig:feature_importance}{{2.13}{23}{\xpg@aux {\xpg@current@opts }{\languagename }Feature permutation importances for classifiers}{figure.caption.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}PyRat: open-source Python library}{24}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}An Alternative to Supervised Classification}{24}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}The PyRAT Pipeline: From Pose to Unsupervised Clustering}{24}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Pose-Estimation as a Foundational Input}{24}{subsection.2.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces \xpg@aux {}{spanish}The foundational input for the PyRAT library: pose-estimation data.}}{25}{figure.caption.26}\protected@file@percent }
\newlabel{fig:pose_input}{{2.14}{25}{\xpg@aux {\xpg@current@opts }{\languagename }The foundational input for the PyRAT library: pose-estimation data}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Unsupervised Behavior Classification}{25}{subsection.2.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces \xpg@aux {}{spanish}The unsupervised behavior classification process in PyRAT. (A) A t-SNE plot visualizing the automatically discovered clusters. (B) A histogram showing the number of frames in each cluster. (C) A dendrogram from the hierarchical clustering, with example frames from the five identified behavioral classes. This figure provides a complete overview of the unsupervised discovery workflow.}}{26}{figure.caption.27}\protected@file@percent }
\newlabel{fig:unsupervised_clustering}{{2.15}{26}{\xpg@aux {\xpg@current@opts }{\languagename }The unsupervised behavior classification process in PyRAT. (A) A t-SNE plot visualizing the automatically discovered clusters. (B) A histogram showing the number of frames in each cluster. (C) A dendrogram from the hierarchical clustering, with example frames from the five identified behavioral classes. This figure provides a complete overview of the unsupervised discovery workflow}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Kinematic Analysis and Integration with Neural Data}{26}{subsection.2.6.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces \xpg@aux {}{spanish}Kinematic analysis in PyRAT. (B) Trajectory plots showing the path of a rat over three consecutive days, with color indicating time. (C) Occupancy heatmaps showing the average time spent in each location. These are standard, essential analyses for behavioral studies.}}{27}{figure.caption.28}\protected@file@percent }
\newlabel{fig:kinematics}{{2.16}{27}{\xpg@aux {\xpg@current@opts }{\languagename }Kinematic analysis in PyRAT. (B) Trajectory plots showing the path of a rat over three consecutive days, with color indicating time. (C) Occupancy heatmaps showing the average time spent in each location. These are standard, essential analyses for behavioral studies}{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces \xpg@aux {}{spanish}Integration of behavioral and neural data. (A) The \texttt  {SignalSubset} function uses the timestamps from a behavioral cluster to extract corresponding segments of LFP data. (B) The \texttt  {SpacialNeuralActivity} function creates a heatmap of neural firing rate as a function of the animal's position in the arena. (Based on Figure 4 from De Almeida et al., 2022).}}{28}{figure.caption.29}\protected@file@percent }
\newlabel{fig:neural_integration}{{2.17}{28}{\xpg@aux {\xpg@current@opts }{\languagename }Integration of behavioral and neural data. (A) The \texttt {SignalSubset} function uses the timestamps from a behavioral cluster to extract corresponding segments of LFP data. (B) The \texttt {SpacialNeuralActivity} function creates a heatmap of neural firing rate as a function of the animal's position in the arena. (Based on Figure 4 from De Almeida et al., 2022)}{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.6}Supervised vs. Unsupervised Approaches: A Complementary View}{28}{subsection.2.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}3D-POP - An automated annotation approach for bird behavior}{29}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}The Data Bottleneck in Animal Pose-Estimation}{29}{subsection.2.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}A Novel Solution: Semi-Automated Annotation via Motion Capture}{30}{subsection.2.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}The Hybrid Mo-Cap and Video Setup}{30}{subsection.2.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces \xpg@aux {}{spanish}The experimental setup for the 3D-POP dataset. (A) The layout of the arena with motion tracking infrared cameras and synchronized RGB cameras. (B) A pigeon subject with reflective markers attached to a head mount and a backpack. This hybrid setup is the key to the semi-automated annotation method. (Based on Figure 2 from Naik et al., 2022).}}{30}{figure.caption.30}\protected@file@percent }
\newlabel{fig:setup}{{2.18}{30}{\xpg@aux {\xpg@current@opts }{\languagename }The experimental setup for the 3D-POP dataset. (A) The layout of the arena with motion tracking infrared cameras and synchronized RGB cameras. (B) A pigeon subject with reflective markers attached to a head mount and a backpack. This hybrid setup is the key to the semi-automated annotation method. (Based on Figure 2 from Naik et al., 2022)}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.4}The Annotation Principle: From Markers to Morphological Keypoints}{30}{subsection.2.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.5}From Manual Annotation to Automated Propagation}{31}{subsection.2.7.5}\protected@fi